\section{Result}
For preparing the data for training and testing, we split the data in \cite{013} by 2 sets: training and testing. For the training set, we have 9000 projects. We use 305 projects as the testing data set. We use the same training and testing set used in \cite{013} in their research work. To get the label as star information for each projects, we used Github API to make queries for getting projects' metadata. In other words, we use source code from 2018 to predict the star in April, 2020.  We focus on 3 Research Questions:
\begin{enumerate}
    \item \textbf{Research Question (RQ) 1}: How well of the popularity level classification?
     \item \textbf{RQ 2}: How well of the exact star prediction using regression models?
      \item \textbf{RQ 3}: Can hyper parameter tuning improve the accuracy?
\end{enumerate}



\subsection{RQ 1: Accuracy on Popularity Level Classification}

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[]
\begin{tabular}{|l|l|l|}
\hline
{\color[HTML]{000000} \textbf{No}} & {\color[HTML]{000000} \textbf{ML Model}} & {\color[HTML]{000000} \textbf{Accuracy (\%)}}        \\ \hline
{\color[HTML]{000000} 1}           & {\color[HTML]{000000} GNB}               & {\color[HTML]{000000} 22.70} \\ \hline
{\color[HTML]{000000} 2}           & {\color[HTML]{000000} LR}                & {\color[HTML]{000000} 28.62}                         \\ \hline
{\color[HTML]{000000} 3}           & {\color[HTML]{000000} DT}                & {\color[HTML]{000000} 22.37}                         \\ \hline
{\color[HTML]{000000} 4}           & {\color[HTML]{000000} RF}                & {\color[HTML]{000000} 21.05}                         \\ \hline
{\color[HTML]{000000} 5}           & {\color[HTML]{000000} AB}                & {\color[HTML]{000000} 22.70}                         \\ \hline
{\color[HTML]{000000} 6}           & {\color[HTML]{000000} LDA}               & {\color[HTML]{000000} 29.61}                         \\ \hline
{\color[HTML]{000000} 7}           & {\color[HTML]{000000} QDA}               & {\color[HTML]{000000} 25.99}                         \\ \hline
{\color[HTML]{000000} \textbf{8}}  & {\color[HTML]{000000} \textbf{SVC}}      & {\color[HTML]{000000} \textbf{32.89}}                \\ \hline
{\color[HTML]{000000} 9}           & {\color[HTML]{000000} MLP}               & {\color[HTML]{000000} 30.26}                         \\ \hline
{\color[HTML]{000000} 10}          & {\color[HTML]{000000} GBo}               & {\color[HTML]{000000} 28.62}                         \\ \hline
\end{tabular}
\label{tblRQ1}
\caption{Github Project Popularity Level Classification Result}
\end{table}

The result for popularity level prediction is shown on Table \ref{tblRQ1}. We use the accuracy function in \cite{010} to compare between ML models. We tried with 10 different ML models. In these models, we used default configurations by \cite{010}. The result shows challenges of the prediction based on the source code only. We achieved the highest accuracy on Support Vector Machine, while lowest on Random Forest. The Neural Network achieved good results compared to other ML models. The Gaussian Naive Bayes seems to have the best running time, but it achieved low accuracy. This result shows that the Random Forest might need to have parameter tuning in future works.


\subsection{RQ 2: Accuracy on Popularity Prediction}

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[]
\begin{tabular}{|l|l|l|}
\hline
{\color[HTML]{000000} \textbf{No}} & {\color[HTML]{000000} \textbf{ML Model}} & {\color[HTML]{000000} \textbf{MAE}}    \\ \hline
{\color[HTML]{000000} 1}           & {\color[HTML]{000000} DTR}               & {\color[HTML]{000000} 1111.02}         \\ \hline
{\color[HTML]{000000} 2}           & {\color[HTML]{000000} RFR}               & {\color[HTML]{000000} 878.68}          \\ \hline
{\color[HTML]{000000} 3}           & {\color[HTML]{000000} ABR}               & {\color[HTML]{000000} 4366.48}         \\ \hline
{\color[HTML]{000000} 4}           & {\color[HTML]{000000} XGBR}              & {\color[HTML]{000000} 657.27}          \\ \hline
{\color[HTML]{000000} \textbf{5}}  & {\color[HTML]{000000} \textbf{LSVR}}     & {\color[HTML]{000000} \textbf{604.02}} \\ \hline
{\color[HTML]{000000} 6}           & {\color[HTML]{000000} MLPR}              & {\color[HTML]{000000} 786.77}          \\ \hline
{\color[HTML]{000000} 7}           & {\color[HTML]{000000} GBR}               & {\color[HTML]{000000} 836.53}          \\ \hline
\end{tabular}
\label{tblRQ2}
\caption{Github Project Popularity Prediction Result}

\end{table}

From result showed in Table \ref{tblRQ2}, we can have some observations. We use Mean Absolute Error (MAE), which is a popular metric in ML regression to check the correlation between predicted result and expected result. In our research problem, it is the number of stars for each project. First, the predicted stars seems to be very high compared to the expected results in many projects. One of possible reason is that the training data contains many project with extremely high stars with more than 50000 stars, which can lead to imbalanced data problem. Second, the Linear Support Vector Machine and the XGBoost Regressor tends to have the best MAE although they are also worst in running time. This shows the potential to improve the accuracy by tuning parameters, which we show in the last RQ.

\subsection{RQ 3: Tuning Hyper Parameter Results}
\subsubsection{RQ 3.1: Tuning result on Support Vector Machine Classification}
We observe the result in Table \ref{tblRQ31}. It shows that the best kernel function is also the default kernel function. One possible reason is that the Github projects' stars might be in Gaussian distribution. Other reason may be the other two kernel functions are used better in neural network than SVM. 

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[]
\small
\begin{tabular}{|l|l|l|l|l|}
\hline
{\color[HTML]{000000} \textbf{Metric}} & {\color[HTML]{000000} \textbf{Range}}                 & {\color[HTML]{000000} \textbf{Best Param}} & {\color[HTML]{000000} \textbf{Best Acc}}         & {\color[HTML]{000000} \textbf{Origin Acc}}       \\ \hline
{\color[HTML]{000000} C}               & {\color[HTML]{000000} 1}                              & {\color[HTML]{000000} 1}                   & {\color[HTML]{000000} }                          & {\color[HTML]{000000} }                          \\ \cline{1-3}
{\color[HTML]{000000} kernel}          & {\color[HTML]{000000} {[}'rbf', 'poly', 'sigmoid'{]}} & {\color[HTML]{000000} rbf}                 & \multirow{-2}{*}{{\color[HTML]{000000} 32.89\%}} & \multirow{-2}{*}{{\color[HTML]{000000} 32.89\%}} \\ \hline
\end{tabular}
\label{tblRQ31}
\caption{Result on Support Vector Machine Classification's tuning}
\end{table}

\subsubsection{RQ 3.2: Tuning result on XGBoost Regressor}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[]
\small
\begin{tabular}{|l|l|l|l|l|}
\hline
{\color[HTML]{000000} \textbf{Metric}}   & {\color[HTML]{000000} \textbf{Range}} & {\color[HTML]{000000} \textbf{Best Param}} & {\color[HTML]{000000} \textbf{Best MAE}}        & {\color[HTML]{000000} \textbf{Default MAE}}     \\ \hline
{\color[HTML]{000000} objective}         & {\color[HTML]{000000} reg:linear}     & {\color[HTML]{000000} reg:linear}          & {\color[HTML]{000000} }                         & {\color[HTML]{000000} }                         \\ \cline{1-3}
{\color[HTML]{000000} colsample\_bytree} & {\color[HTML]{000000} 0.3}            & {\color[HTML]{000000} 0.3}                 & {\color[HTML]{000000} }                         & {\color[HTML]{000000} }                         \\ \cline{1-3}
{\color[HTML]{000000} learning\_rate}    & {\color[HTML]{000000} {[}0.1,1{]}}    & {\color[HTML]{000000} 0.1}                 & {\color[HTML]{000000} }                         & {\color[HTML]{000000} }                         \\ \cline{1-3}
{\color[HTML]{000000} max\_depth}        & {\color[HTML]{000000} {[} 1,3,5{]}}   & {\color[HTML]{000000} 5}                   & {\color[HTML]{000000} }                         & {\color[HTML]{000000} }                         \\ \cline{1-3}
{\color[HTML]{000000} alpha}             & {\color[HTML]{000000} {[}10,20{]}}    & {\color[HTML]{000000} 10}                  & {\color[HTML]{000000} }                         & {\color[HTML]{000000} }                         \\ \cline{1-3}
{\color[HTML]{000000} n\_estimators}     & {\color[HTML]{000000} {[}10,100{]}}   & {\color[HTML]{000000} 10}                  & \multirow{-6}{*}{{\color[HTML]{000000} 648.02}} & \multirow{-6}{*}{{\color[HTML]{000000} 657.27}} \\ \hline
\end{tabular}
\label{tblRQ32}
\caption{Result on XGBoost Regression's Tuning}
\end{table}

The result of tuning XGBoost is shown in \ref{tblRQ32}. From this result, we can see that if we make hyper parameter tuning, the MAE can be better significantly. The algorithm tends to work better with lower learning rate. For the max depth, the higher max depth brings better result, since the XGBoost can learn more information. One of surprised thing we see is that with low number of estimator, we achieve better MAE. Higher number of estimators caused significantly increasing in performance, so we expected the XGBoost will get better accuracy with large size estimator. The result reveals that large number of estimators doesn't mean increasing the accuracy.

