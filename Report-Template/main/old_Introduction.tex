\section{Introduction}
Deep Learning (DL) has been applied in different Software Engineering (SE) researches and problems\cite{001,002,003}. It can contribute to all stages of software development life cycle, from requirements extraction, design, implementation to maintenance \cite{001}. Since DL was vastly applied earlier in Natural Language Processing (NLP), a popular trend of applying DL in SE is to consider the input of SE problems as different types of documentation similar to NLP input. According to \cite{003}, there are 3 main types of documentation: Natural Language (NL), Software Documentation (SD) and Programming Language (PL). Based on the requirement of each tasks, the output of research works that used these types of documentation as input is varied by different types of code tokens. For the research works that used NL as input, they tend to find the element of code environment that satisfied the description in NL, which they applied for code search \cite{004,005} and code synthesis \cite{006}. SD is special type of documentation written in NL but contains information about description of the Application Programming Interfaces (APIs) of different programming languages. There is a work in the literature on representing APIs as vector from SD \cite{007}. For the research works used PL as input, they used deep learning translation between PLs \cite{009} and code suggestion \cite{008}.

The output of DL researches in SE problems can not only be the source code or code tokens, but also could be the information from SDs and NLs.  Different applications are proposed to translate between each types of documentations using Machine Learning (ML). There are works on generating SD as pseudo-code from code using DL and ML \cite{010,011}, or generating documentation from API specifications \cite{012}, or generating commit messages in NL \cite{013}. Neural Machine Translation (NMT), which is a technique relied on advantages of DL, can be assumed as the best translation engine for SE. The ability of NMT relies on the formation on multiple layers of neural network to capture more information for the translation of each elements in the source language \cite{014}. Besides, along with text sequence, NMT can be applicable on a different data structure such as graph or tree, which is suitable for the representation of code \cite{015}. Another advantage of NMT is the performance for inferring the results, which is usually outperform earlier Machine Translation techniques \cite{014}.

Before the era of NMT, Statistical Machine Translation (SMT) \cite{016} was the most popular technique for solving SE problems which relied on MT approaches. With the idea of extending the original Bayes rule \cite{017}, SMT provides the ability of learning the context in Natural Language for translation between popular languages. Since the source code also embed information of NL \cite{018}, SMT is successfully be applicable of SE problems as translation between versions of Python \cite{019} and between different PLs \cite{020}. However, compared to newer trends of translation engines such as NMT, SMT reveals 2 drawbacks. First, it cannot learn information from long sequence of text. An implementation tool of SMT, Phrasal \cite{021} can only process the phrase with maximum length of 7. Secondly, the training and testing time of SMT become worse with large training data and increases exponentially \cite{022}. For these reasons, NMT has replaced SMT in SE problems \cite{010,011}.

Although having many advantages, NMT itself contains some challenges which also appear in researches of different areas along with SE. \cite{024,023} mention about an important problem of NMT compared to SMT as rare words problems in NLP. Current popular NMT engines, OpenNMT \cite{025} or Google NMT \cite{026}, cannot handle large size vocabulary with more than 100000 words. To optimize the problem, researchers considered rare words as Unknown words which their translated results are not counted to the final results. This fact caused NMT performs poorly when rare/unknown words are frequent in the corpus \cite{024}. In SE researches, the problem of Type Inference using MT shows that SMT model provided by \cite{028} has a significant higher accuracy compared to the original NMT approach in \cite{027}. Similarly, for natural language diacritic restoration, \cite{029} shows that SMT outperforms NMT.  \cite{026,027,028} have the same characteristics of parallel corpus, i.e., the length of source and target pairs are equal and the order of the source and target words are consistent with each other. This leads us to an assumption that if the parallel corpus for training MT has these characteristics, it will affect the accuracy of NMT. In summary, NMT tends to have lower accuracy than SMT due to the methodology of NMT that didn't support the rare words translation and the characteristics of parallel corpus. 

In this work, we further investigate the efficiency of NMT vs. SMT in a new research problem that has similar characteristics of parallel corpus as \cite{026,027,028}. Instead of focusing on only limited types of tokens in code environment, our problem provide a solution to help developers get the code of all types of code tokens based on its first letters in the form of abbreviation/prefix of tokens. To implement the solution for this problem, we build two spaces of abbreviations of code tokens as source language and code tokens as target language. Then, we implement two machine translation models, Neural Machine Translation and Statistical Machine Translation to learn the mapping from prefixes to code tokens for code suggestion. We also analyze the affect of unknown tokens along with the accuracy on each types of tokens. By the evaluation, we show that SMT outperforms the original NMT. We called our approach PrefixMapping and analyze the effective translation on three types of documentation: NL, SD and PL. Overall, this paper provides the following contributions:

\begin{enumerate}
\item Proposing the translation based engine for code completion from first letters of tokens.
\item Providing algorithms for extracting parallel corpus of prefixes and tokens in 3 types of documentation used in NLP and SE.
\item Implementing and Optimizing Statistical Machine Translation for PrefixMapping.
\item Implementing and Optimizing Neural Machine Translation for PrefixMapping.
\item Analyzing the accuracy of NMT compared to SMT along with accuracy depending on each types of code tokens.
\end{enumerate}

The structure of the paper is provided as follow. In the next section, we will describe about our research problem. In this section, we will define important concepts we use for abbreviations, algorithms for data collection, and the overview architecture of the system. The core engines of translation, NMT and SMT, along with their optimization for this problem is described in the third section. In the evaluation, we will provide the accuracy and head-to-head comparison between NMT and SMT in 3 types of documentation. We published our data and code at \cite{053}.



